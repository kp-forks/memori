---
title: Advanced Augmentation
description: How Memori's Advanced Augmentation engine extracts structured facts, preferences, and knowledge from your AI conversations — all stored in your own database.
---

# Advanced Augmentation

Advanced Augmentation is the AI engine inside Memori that turns raw conversations into structured, searchable memories. It runs asynchronously in the background to minimize impact on your response path. All extracted data is stored directly in your database.

## What It Does

When your application has a conversation through a Memori-wrapped LLM client, the augmentation engine:

1. Reads the full conversation (user messages and AI responses)
2. Identifies facts, preferences, skills, attributes, and events
3. Extracts semantic triples (subject-predicate-object relationships)
4. Generates vector embeddings for semantic search
5. Stores everything in your database

No extra code required — just initialize Memori with your database connection and set attribution.

## How It Works

The augmentation flow is fully asynchronous and designed to avoid blocking your main request path.

1. Your app makes an LLM call through the wrapped client
2. Memori returns the response immediately
3. In the background, the conversation is queued for processing
4. The augmentation engine extracts structured memories
5. Memories are stored in your database for future recall

In short-lived scripts, call `mem.augmentation.wait()` to ensure processing completes before exit.

```python
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from memori import Memori
from openai import OpenAI

engine = create_engine("sqlite:///memori.db")
SessionLocal = sessionmaker(bind=engine)

client = OpenAI()
mem = Memori(conn=SessionLocal).llm.register(client)
mem.attribution(entity_id="user_123", process_id="my_agent")
mem.config.storage.build()

# This returns immediately — no augmentation delay
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {"role": "user", "content": "I love hiking in the mountains."}
    ]
)
print(response.choices[0].message.content)

# Only needed in short-lived scripts
mem.augmentation.wait()
```

## Extraction Types

| Type                   | What it captures                                        | Scope                                |
| ---------------------- | ------------------------------------------------------- | ------------------------------------ |
| **Facts**              | Objective information with vector embeddings            | Per entity — shared across processes |
| **Preferences**        | User choices, opinions, and tastes                      | Per entity                           |
| **Skills & Knowledge** | Abilities and expertise levels                          | Per entity                           |
| **Attributes**         | Process-level information about what your agent handles | Per process                          |

**Database tables:**

| Table                         | Purpose                                      |
| ----------------------------- | -------------------------------------------- |
| `memori_conversation`         | Stores conversations                         |
| `memori_conversation_message` | Individual messages within conversations     |
| `memori_session`              | Groups related LLM interactions              |
| `memori_entity`               | Entities (users, organizations, etc.)        |
| `memori_process`              | Processes (agents, bots, workflows)          |
| `memori_entity_fact`          | Extracted facts with vector embeddings       |
| `memori_process_attribute`    | Process-level attributes and characteristics |

## Semantic Triples

Advanced Augmentation uses named-entity recognition to extract semantic triples (subject, predicate, object). These form the building blocks of the [Knowledge Graph](/docs/memori-byodb/concepts/knowledge-graph).

**Example** — from _"My favorite database is PostgreSQL and I use it with FastAPI"_:

| Subject | Predicate         | Object               |
| ------- | ----------------- | -------------------- |
| user    | favorite_database | PostgreSQL           |
| user    | uses              | FastAPI              |
| user    | uses_with         | PostgreSQL + FastAPI |

Memori automatically deduplicates triples — if the same fact is mentioned multiple times, it increments the mention count and updates the timestamp.

**Database tables:** `memori_subject`, `memori_predicate`, `memori_object`, `memori_knowledge_graph`

## Embeddings

Vector embeddings are created using the **all-mpnet-base-v2** sentence transformer with **768 dimensions**. These embeddings power the semantic search used for context recall via cosine similarity.

## Context Recall

When a query is sent to an LLM through a wrapped client, Memori automatically:

1. Intercepts the outbound LLM call
2. Uses semantic search to find entity facts matching the query
3. Passes vector embeddings to FAISS for similarity ranking
4. Injects the top-N most relevant facts into the system prompt
5. Forwards the enriched request to the LLM provider

## Schema ERD

![Memori Schema ERD](https://s3.us-east-1.amazonaws.com/images.memorilabs.ai/erd.png)

## When to Use It

- Chatbots or AI assistants with returning users
- Use cases that need to remember user preferences across sessions
- You want personalized AI interactions
- With multi-step workflows or agentic systems
- For building relationships between entities

<Admonition type="important" title="Attribution is Required">
  For Memori to provide all Advanced Augmentation capabilities, attribution must
  be set before making LLM calls. Without attribution, Memori cannot create or
  recall memories.
</Admonition>
